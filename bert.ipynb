{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478ccfd5-754d-499f-a287-a64c0b7e9141",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers datasets torch scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81c96b29-1cdb-43aa-85ee-d346c4882472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU disponível: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Verifica se há uma GPU disponível\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU disponível: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"GPU não disponível, usando CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8d9c46d4-1d40-428e-843b-08a424705649",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from tqdm import tqdm  # Para barra de progresso\n",
    "from collections import Counter\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c9be9a6-ffc0-4e93-99bb-7bb63329d74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Carregar o dataset\n",
    "# Aqui estamos usando o dataset de reviews do IMDB como exemplo.\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba268ba1-102f-41b5-ae46-98296b865edb",
   "metadata": {},
   "source": [
    "## brief exploratory analysis of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f030586d-5aae-4c2d-996a-e85a61d6c048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['neg', 'pos'], id=None)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d038eaa2-f8a5-4265-8971-b5c6ab8faea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f04a9a3-e9df-4b5c-a10f-ab77629f1292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribuição de classes: Counter({0: 12500, 1: 12500})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "labels = dataset[\"train\"][\"label\"]\n",
    "label_counts = Counter(labels)\n",
    "print(\"Distribuição de classes:\", label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95b9eedb-5e35-41a7-aec7-848871aa6b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comprimento médio: 233.79 palavras\n"
     ]
    }
   ],
   "source": [
    "text_lengths = [len(text.split()) for text in dataset[\"train\"][\"text\"]]\n",
    "print(f\"Comprimento médio: {sum(text_lengths) / len(text_lengths):.2f} palavras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "079c6817-e4a8-444f-9077-944b9f511544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comprimento máximo: 2470 palavras\n"
     ]
    }
   ],
   "source": [
    "max_length = max(len(text.split()) for text in dataset[\"train\"][\"text\"])\n",
    "print(f\"Comprimento máximo: {max_length} palavras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f386560-520d-480a-abc6-f6e0e6afca8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Textos vazios: 0\n"
     ]
    }
   ],
   "source": [
    "empty_texts = [text for text in dataset[\"train\"][\"text\"] if len(text.strip()) == 0]\n",
    "print(f\"Textos vazios: {len(empty_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e056893-90e4-4c60-9e32-03d8d9732e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Textos inválidos: 0\n"
     ]
    }
   ],
   "source": [
    "invalid_texts = [text for text in dataset[\"train\"][\"text\"] if not any(char.isalpha() for char in text)]\n",
    "print(f\"Textos inválidos: {len(invalid_texts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9435d805-3c12-4a32-9aa6-59ea5860ece2",
   "metadata": {},
   "source": [
    "## Instantiating the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25b62ee4-a09d-4db4-96c2-0fc81b76b6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Carregar o tokenizer do BERT\n",
    "# O tokenizer converte texto em tokens compatíveis com o modelo BERT.\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa71bd9b-c892-47f9-aa77-be51a2a2870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para tokenizar os textos\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# Aplicar a tokenização no dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d8928ab-dfcd-4515-82a7-d7bd3780af6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Preparar o dataset para o treinamento\n",
    "# O Hugging Face exige colunas específicas: 'input_ids' e 'attention_mask'.\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])  # Remove o texto original\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")  # Renomeia a coluna alvo\n",
    "tokenized_datasets.set_format(\"torch\")  # Converte para tensores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b8f718e-a48f-4003-a210-e62a3c06d6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir em treino e validação\n",
    "train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(2000))  # Exemplo reduzido\n",
    "test_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(500))    # Exemplo reduzido\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f113be60-199f-4e5e-996e-44a41842f8d4",
   "metadata": {},
   "source": [
    "## Instantiating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89732f07-8027-4fba-8731-e6ee4f003df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 4. Carregar o modelo pré-treinado\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3452136e-587e-4cd1-bdf2-ed81a69fa9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Definir métricas para avaliação\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"binary\")\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c66cf9d-f568-48f0-ac71-c1d06e6338e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leticia/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 6. Configurar o treinamento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954b8587-a00a-4bbe-8689-d683fb49814c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install accelerate>=0.26.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bde93f57-0b4a-45dc-9d07-8f01c799fecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c9bb8a9-f8c7-44fe-ae56-e6ec27144faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 03:36, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.327600</td>\n",
       "      <td>0.359210</td>\n",
       "      <td>0.878000</td>\n",
       "      <td>0.883365</td>\n",
       "      <td>0.833935</td>\n",
       "      <td>0.939024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.251000</td>\n",
       "      <td>0.379327</td>\n",
       "      <td>0.904000</td>\n",
       "      <td>0.899582</td>\n",
       "      <td>0.926724</td>\n",
       "      <td>0.873984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=0.2963378931283951, metrics={'train_runtime': 217.1611, 'train_samples_per_second': 18.42, 'train_steps_per_second': 2.302, 'total_flos': 1052444221440000.0, 'train_loss': 0.2963378931283951, 'epoch': 2.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. Treinar o modelo\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93f0b8c3-23a3-4110-972d-1dbc88e0876f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3793271481990814, 'eval_accuracy': 0.904, 'eval_f1': 0.899581589958159, 'eval_precision': 0.9267241379310345, 'eval_recall': 0.8739837398373984, 'eval_runtime': 7.0195, 'eval_samples_per_second': 71.231, 'eval_steps_per_second': 8.975, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "# 8. Avaliar o modelo\n",
    "metrics = trainer.evaluate()\n",
    "print(metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a0002f2-4f0e-45f3-9ce7-e18477af2c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Certificar-se de que o modelo está na GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb43408-5d63-4894-a834-c42785aacd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Fazer previsões com novos textos\n",
    "test_texts = [\"This movie was amazing!\", \"I hated this movie.\"]\n",
    "test_encodings = tokenizer(test_texts, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "output = model(**test_encodings)\n",
    "predictions = output.logits.argmax(dim=1)\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "faa0bc6e-1d85-424d-ac5b-e86856e1723c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo no dispositivo: cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Modelo no dispositivo: {next(model.parameters()).device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "274460e2-5271-40d6-9fad-ecbedb95afdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar entradas\n",
    "test_texts = [\"This movie was amazing!\", \"I hated this movie.\", \"I don't think the movie is really good\"]\n",
    "inputs = tokenizer(test_texts, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "# Mover os inputs para o mesmo dispositivo do modelo\n",
    "inputs = {key: value.to(device) for key, value in inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6fc46de5-ef7d-4ccc-a913-e3ecff19e2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Fazer a inferência\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Obter as previsões\n",
    "predictions = outputs.logits.argmax(dim=1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28507eab-1bf6-4ff5-8557-a1bec0c4e4c7",
   "metadata": {},
   "source": [
    "## Question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "28eaa609-34cb-434e-b68b-52da88541022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b425931c5f4a4ce589b0ec71c63a387b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/8.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3db8f5e2159b493f8b9f78df3200a279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/16.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e206786b882b4a0286089c760ef7b347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/1.35M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb1d6c2b2e924c2f8d6b12fe7c87b50e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/130319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14c928d6a8de4fc484c882c0cd57d955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/11873 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '56be85543aeaaa14008c9063', 'title': 'Beyoncé', 'context': 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".', 'question': 'When did Beyonce start becoming popular?', 'answers': {'text': ['in the late 1990s'], 'answer_start': [269]}}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Carregar dataset\n",
    "squad = load_dataset(\"squad_v2\")\n",
    "\n",
    "# Visualizar exemplos\n",
    "print(squad[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34214bcd-0563-4460-a945-57780f23f288",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    inputs = tokenizer(\n",
    "        examples[\"question\"], examples[\"context\"], truncation=True, padding=True, max_length=512\n",
    "    )\n",
    "    return inputs\n",
    "\n",
    "tokenized_squad = squad.map(preprocess_data, batched=True)\n",
    "\n",
    "# Configurar o treinamento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\", evaluation_strategy=\"epoch\", learning_rate=2e-5, num_train_epochs=3\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_squad[\"train\"],\n",
    "    eval_dataset=tokenized_squad[\"validation\"],\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e335af0f-6b92-4335-9c59-db3e176ed007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07a456a2b9954faf8f7a7d412f0db326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/451 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0438786c68e40f9bbc4fc0078ee0451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/265M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77d35e857349479092fef9e4156a4461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9f0946292f14e3f98edbf840881dfe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1b3e1c96de54b6b85bee60cf8195a9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "de6dd3cc-d260-42a9-9ba5-1fb4736014e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.14506877958774567, 'start': 10, 'end': 29, 'answer': 'modelo de linguagem'}\n"
     ]
    }
   ],
   "source": [
    "question = \"O que é BERT?\"\n",
    "context = \"BERT é um modelo de linguagem desenvolvido pela Google, usado para tarefas de NLP.\"\n",
    "\n",
    "result = qa_pipeline(question=question, context=context)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef10427-74c4-4d9e-8fee-a148f29937a6",
   "metadata": {},
   "source": [
    "### até aqui funcionando"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f443d2-5f88-4a14-9fd3-4fcf6987cd1f",
   "metadata": {},
   "source": [
    "## Bertimbau com question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca0d5d9-b114-41b4-a25b-670a376cf0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2aec08-a012-424c-a570-2a99ba7b9f22",
   "metadata": {},
   "source": [
    "### traduzindo dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b1e26b7d-b3b8-49f0-82e4-801f18b3c3b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08b3aeaf3ece4a799e5d6ebb05147fa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4528005a9424f54b4556644b9d92c7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "587385ffbd6e4df3bffa0ceee985f918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/1.82M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88cf74de235d4394b7528808734018d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb8a82d8c1a4e2cb9e124a3798c32a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c19f664b-af76-4c70-8f2e-def43f6bc9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = dataset[\"train\"].select(range(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3861d6f3-7f19-4ec0-83cf-9b7b52b4ee0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '5733be284776f41900661182', 'title': 'University_of_Notre_Dame', 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.', 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?', 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}\n"
     ]
    }
   ],
   "source": [
    "print(subset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e2b7788d-77f2-48c0-bdd8-55a54e2ebaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "title\n",
      "context\n",
      "question\n",
      "answers\n"
     ]
    }
   ],
   "source": [
    "dataset[\"train\"]\n",
    "\n",
    "for example in dataset[\"train\"][0:10]:\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211e729e-b9f3-48b0-b6a4-a412e5659194",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install deep_translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8152b332-44c8-4283-91fe-df85afc23c81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10570"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ee7cc170-4f3f-402a-8dd9-e1a64e1aec0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset = dataset[\"train\"].shuffle(seed=42).select(range(int(0.3 * len(dataset[\"train\"]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d87666b-9546-4e8d-a03c-d7a76e2c7f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from deep_translator import GoogleTranslator\n",
    "import numpy as np\n",
    "\n",
    "# Definir a função ensure_serializable antes de usá-la\n",
    "def ensure_serializable(obj):\n",
    "    \"\"\"\n",
    "    Converte qualquer valor não serializável para tipos compatíveis com JSON.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()  # Converter ndarray para lista\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: ensure_serializable(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [ensure_serializable(value) for value in obj]\n",
    "    else:\n",
    "        return obj  # Retornar o valor original se já for serializável\n",
    "\n",
    "# Função para traduzir texto com pausa\n",
    "def translate_text_with_pause(text, source=\"en\", target=\"pt\"):\n",
    "    time.sleep(1)  # Espera de 1 segundo entre as requisições\n",
    "    return GoogleTranslator(source=source, target=target).translate(text)\n",
    "\n",
    "# Traduzir o dataset com salvamento incremental\n",
    "def translate_dataset(dataset, output_file=\"translated_dataset.json\"):\n",
    "    translated_examples = []\n",
    "\n",
    "    # Carregar traduções parciais, se o arquivo já existir\n",
    "    try:\n",
    "        with open(output_file, \"r\") as file:\n",
    "            content = file.read().strip()\n",
    "            if content:\n",
    "                translated_examples = json.loads(content)\n",
    "                print(f\"Carregadas {len(translated_examples)} traduções existentes.\")\n",
    "            else:\n",
    "                print(\"Arquivo vazio encontrado. Começando do zero.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Arquivo de saída não encontrado. Criando um novo.\")\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump([], file)\n",
    "\n",
    "    # Verificar progresso\n",
    "    already_translated = len(translated_examples)\n",
    "    dataset = dataset[already_translated:]  # Ignorar exemplos já traduzidos\n",
    "    print(f\"Traduzindo {len(dataset)} exemplos restantes.\")\n",
    "\n",
    "    # Barra de progresso\n",
    "    for i, example in enumerate(tqdm(dataset, desc=\"Traduzindo\", unit=\"exemplo\")):\n",
    "        try:\n",
    "            # Traduzir contexto e pergunta\n",
    "            context_translated = translate_text_with_pause(example[\"context\"])\n",
    "            question_translated = translate_text_with_pause(example[\"question\"])\n",
    "\n",
    "            # Traduzir todas as respostas\n",
    "            answers_translated = {\n",
    "                \"text\": [translate_text_with_pause(ans) for ans in example[\"answers\"][\"text\"]],\n",
    "                \"answer_start\": [int(start) for start in example[\"answers\"][\"answer_start\"]],  # Converter para int\n",
    "            }\n",
    "\n",
    "            # Adicionar exemplo traduzido\n",
    "            translated_example = {\n",
    "                \"context\": context_translated,\n",
    "                \"question\": question_translated,\n",
    "                \"answers\": answers_translated,\n",
    "            }\n",
    "\n",
    "            # Garantir que todos os valores são serializáveis\n",
    "            translated_example = ensure_serializable(translated_example)\n",
    "            translated_examples.append(translated_example)\n",
    "\n",
    "            # Salvar progresso no arquivo JSON\n",
    "            with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "                json.dump(translated_examples, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao traduzir exemplo {i}: {e}\")\n",
    "            continue  # Pula para o próximo exemplo\n",
    "\n",
    "    print(f\"Tradução concluída. Total traduzido: {len(translated_examples)} exemplos.\")\n",
    "    return translated_examples\n",
    "\n",
    "# Exemplo de uso\n",
    "# Criar subconjunto de treino\n",
    "train_subset = dataset[\"train\"].shuffle(seed=42).select(range(int(0.3 * len(dataset[\"train\"]))))\n",
    "train_subset = train_subset.to_pandas().to_dict(orient=\"records\")\n",
    "print(\"Total de entradas do dataset: \", len(train_subset))\n",
    "\n",
    "# Traduzir o conjunto de treino com barra de progresso\n",
    "translated_train = translate_dataset(train_subset, \"translated_train.json\")\n",
    "\n",
    "\n",
    "val_subset = dataset[\"validation\"].shuffle(seed=42).select(range(int(0.3 * len(dataset[\"validation\"]))))\n",
    "val_subset = val_subset.to_pandas().to_dict(orient=\"records\")\n",
    "# Traduzir o conjunto de validação\n",
    "translated_validation = translate_dataset(val_subset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8031c1-fdb6-4f24-9103-182b17e7f68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from deep_translator import GoogleTranslator\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Função para garantir que os valores são serializáveis\n",
    "def ensure_serializable(obj):\n",
    "    \"\"\"\n",
    "    Converte valores incompatíveis com JSON, como ndarrays, para tipos serializáveis.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, np.ndarray):  # Converte ndarray para lista\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, dict):  # Converte dicionários recursivamente\n",
    "        return {key: ensure_serializable(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):  # Converte listas recursivamente\n",
    "        return [ensure_serializable(value) for value in obj]\n",
    "    else:\n",
    "        return obj  # Retorna o valor original se já for serializável\n",
    "\n",
    "# Função para traduzir texto\n",
    "def translate_text(text, source=\"en\", target=\"pt\"):\n",
    "    try:\n",
    "        return GoogleTranslator(source=source, target=target).translate(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao traduzir texto: {text}\\n{e}\")\n",
    "        return text  # Retorna o texto original em caso de falha\n",
    "\n",
    "# Função para tradução paralela de um conjunto de textos\n",
    "def parallel_translate(texts, max_workers=4):\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        translations = list(tqdm(executor.map(translate_text, texts), total=len(texts), desc=\"Traduzindo textos\"))\n",
    "    return translations\n",
    "\n",
    "# Função para traduzir o dataset\n",
    "def translate_dataset(dataset, output_file=\"translated_dataset.json\", max_workers=4):\n",
    "    translated_examples = []\n",
    "\n",
    "    # Carregar traduções parciais de forma robusta\n",
    "    try:\n",
    "        with open(output_file, \"r\", encoding=\"utf-8\") as file:\n",
    "            content = file.read().strip()  # Remover espaços em branco\n",
    "            if content:  # Verifica se o arquivo não está vazio\n",
    "                translated_examples = json.loads(content)\n",
    "                print(f\"Carregadas {len(translated_examples)} traduções existentes.\")\n",
    "            else:\n",
    "                print(\"Arquivo vazio encontrado. Começando do zero.\")\n",
    "                translated_examples = []\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        print(\"Arquivo inexistente ou corrompido. Começando do zero.\")\n",
    "        translated_examples = []\n",
    "\n",
    "    # Verificar progresso\n",
    "    already_translated = len(translated_examples)\n",
    "    dataset = dataset[already_translated:]  # Ignorar exemplos já traduzidos\n",
    "    print(f\"Traduzindo {len(dataset)} exemplos restantes.\")\n",
    "\n",
    "    # Preparar os campos para tradução paralela\n",
    "    contexts = [example[\"context\"] for example in dataset]\n",
    "    questions = [example[\"question\"] for example in dataset]\n",
    "    answers = [example[\"answers\"][\"text\"] for example in dataset]\n",
    "\n",
    "    # Traduzir paralelamente os campos\n",
    "    print(\"Traduzindo contextos...\")\n",
    "    translated_contexts = parallel_translate(contexts, max_workers=max_workers)\n",
    "\n",
    "    print(\"Traduzindo perguntas...\")\n",
    "    translated_questions = parallel_translate(questions, max_workers=max_workers)\n",
    "\n",
    "    print(\"Traduzindo respostas...\")\n",
    "    translated_answers = [parallel_translate(answer, max_workers=max_workers) for answer in answers]\n",
    "\n",
    "    # Reconstruir os exemplos traduzidos\n",
    "    for i in range(len(dataset)):\n",
    "        translated_example = {\n",
    "            \"context\": translated_contexts[i],\n",
    "            \"question\": translated_questions[i],\n",
    "            \"answers\": {\n",
    "                \"text\": translated_answers[i],\n",
    "                \"answer_start\": ensure_serializable(dataset[i][\"answers\"][\"answer_start\"]),  # Serializável\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # Garantir que todo o exemplo é serializável\n",
    "        translated_example = ensure_serializable(translated_example)\n",
    "        translated_examples.append(translated_example)\n",
    "\n",
    "        # Salvamento incremental\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(translated_examples, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Tradução concluída. Total traduzido: {len(translated_examples)} exemplos.\")\n",
    "    return translated_examples\n",
    "\n",
    "# Carregar o SQuAD\n",
    "dataset = load_dataset(\"squad\")\n",
    "\n",
    "# Selecionar um subconjunto (opcional)\n",
    "train_subset = dataset[\"train\"].shuffle(seed=42).select(range(500))  # Traduzir 500 exemplos\n",
    "train_subset = train_subset.to_pandas().to_dict(orient=\"records\")\n",
    "\n",
    "# Traduzir o conjunto de treino com paralelismo\n",
    "translated_train = translate_dataset(train_subset, output_file=\"translated_train.json\", max_workers=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed586e5d-1abd-4b41-a789-7cc41c5e05e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from deep_translator import GoogleTranslator\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Função para garantir que os valores são serializáveis\n",
    "def ensure_serializable(obj):\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: ensure_serializable(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [ensure_serializable(value) for value in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Salvamento seguro\n",
    "def safe_save_json(data, output_file):\n",
    "    temp_file = output_file + \".tmp\"\n",
    "    with open(temp_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "    os.replace(temp_file, output_file)\n",
    "\n",
    "# Função para traduzir texto\n",
    "def translate_text(text, source=\"en\", target=\"pt\"):\n",
    "    try:\n",
    "        return GoogleTranslator(source=source, target=target).translate(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao traduzir texto: {text}\\n{e}\")\n",
    "        return text\n",
    "\n",
    "# Tradução paralela\n",
    "def parallel_translate(texts, max_workers=4):\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        translations = list(tqdm(executor.map(translate_text, texts), total=len(texts), desc=\"Traduzindo textos\"))\n",
    "    return translations\n",
    "\n",
    "# Função principal de tradução\n",
    "def translate_dataset_in_batches(dataset, output_file=\"translated_dataset.json\", max_workers=4, batch_size=500, max_samples=5000):\n",
    "    # Carregar traduções parciais\n",
    "    try:\n",
    "        with open(output_file, \"r\", encoding=\"utf-8\") as file:\n",
    "            content = file.read().strip()  # Verifica se o arquivo está vazio\n",
    "            if content:\n",
    "                translated_examples = json.loads(content)\n",
    "                print(f\"Carregadas {len(translated_examples)} traduções existentes.\")\n",
    "            else:\n",
    "                print(\"Arquivo vazio encontrado. Começando do zero.\")\n",
    "                translated_examples = []\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        print(\"Arquivo inexistente ou corrompido. Começando do zero.\")\n",
    "        translated_examples = []\n",
    "\n",
    "    # Determinar quantos exemplos já foram traduzidos\n",
    "    already_translated = len(translated_examples)\n",
    "    if already_translated >= max_samples:\n",
    "        print(f\"Já foram traduzidos {already_translated} exemplos. Nenhuma tradução adicional necessária.\")\n",
    "        return translated_examples\n",
    "\n",
    "    # Traduzir em lotes\n",
    "    for start in range(already_translated, min(max_samples, len(dataset)), batch_size):\n",
    "        end = min(start + batch_size, max_samples)\n",
    "        print(f\"Traduzindo exemplos {start} a {end}...\")\n",
    "\n",
    "        # Preparar lote atual\n",
    "        batch = dataset[start:end]\n",
    "        contexts = [example[\"context\"] for example in batch]\n",
    "        questions = [example[\"question\"] for example in batch]\n",
    "        answers = [example[\"answers\"][\"text\"] for example in batch]\n",
    "\n",
    "        # Traduzir os campos\n",
    "        translated_contexts = parallel_translate(contexts, max_workers=max_workers)\n",
    "        translated_questions = parallel_translate(questions, max_workers=max_workers)\n",
    "        translated_answers = [parallel_translate(answer, max_workers=max_workers) for answer in answers]\n",
    "\n",
    "        # Reconstruir exemplos traduzidos\n",
    "        for i in range(len(batch)):\n",
    "            translated_example = {\n",
    "                \"context\": translated_contexts[i],\n",
    "                \"question\": translated_questions[i],\n",
    "                \"answers\": {\n",
    "                    \"text\": translated_answers[i],\n",
    "                    \"answer_start\": ensure_serializable(batch[i][\"answers\"][\"answer_start\"]),\n",
    "                },\n",
    "            }\n",
    "            translated_example = ensure_serializable(translated_example)\n",
    "            translated_examples.append(translated_example)\n",
    "\n",
    "        # Salvamento incremental\n",
    "        safe_save_json(translated_examples, output_file)\n",
    "\n",
    "        print(f\"Tradução de {end} exemplos concluída. Total traduzido até agora: {len(translated_examples)} exemplos.\")\n",
    "\n",
    "        # Verifica se o limite foi atingido\n",
    "        if len(translated_examples) >= max_samples:\n",
    "            break\n",
    "\n",
    "    print(f\"Tradução concluída. Total traduzido: {len(translated_examples)} exemplos.\")\n",
    "    return translated_examples\n",
    "\n",
    "# Carregar o SQuAD\n",
    "dataset = load_dataset(\"squad\")[\"train\"].to_pandas().to_dict(orient=\"records\")  # Carregar como lista de dicionários\n",
    "\n",
    "# Traduzir em lotes até atingir 5.000 exemplos\n",
    "translated_train = translate_dataset_in_batches(\n",
    "    dataset,\n",
    "    output_file=\"translated_train.json\",\n",
    "    max_workers=8,\n",
    "    batch_size=500,\n",
    "    max_samples=5000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "69701d34-3ec1-470e-95bd-c9cfb652f771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'=0.26.0'\t    huggingread.txt\t     LLM\n",
      " api_key.txt\t    hugging.txt\t\t     results\n",
      " bert.ipynb\t    langchain_test.ipynb     translated_train.json\n",
      " first_test.ipynb   langchain_to_git.ipynb\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "976970ef-30c0-49f4-a22f-736acaeca5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context': 'O Pew Forum on Religion & Public Life classifica o Egito como o quinto pior país do mundo em liberdade religiosa. A Comissão dos Estados Unidos sobre Liberdade Religiosa Internacional, uma agência independente bipartidária do governo dos EUA, colocou o Egito em sua lista de observação de países que exigem monitoramento rigoroso devido à natureza e extensão das violações da liberdade religiosa praticadas ou toleradas pelo governo. De acordo com uma pesquisa Pew Global Attitudes de 2010, 84% dos egípcios entrevistados apoiaram a pena de morte para aqueles que abandonam o islamismo; 77% apoiaram chicotadas e cortes de mãos por roubo e furto; e 82% apoiam o apedrejamento de uma pessoa que comete adultério.', 'question': 'Qual a porcentagem de egípcios entrevistados que apoiam a pena de morte para aqueles que abandonam o islamismo?', 'answers': {'text': ['84%'], 'answer_start': [468]}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Carregar o dataset traduzido\n",
    "with open(\"translated_train.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Exemplo de estrutura de um dado\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "912dfcb5-cf44-46bb-b86f-ab1846fa4229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino: 4500 exemplos, Validação: 500 exemplos\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Converter para o formato do datasets\n",
    "dataset = Dataset.from_dict({\n",
    "    \"context\": [example[\"context\"] for example in data],\n",
    "    \"question\": [example[\"question\"] for example in data],\n",
    "    \"answers\": [example[\"answers\"] for example in data],\n",
    "})\n",
    "\n",
    "# Dividir em treino e validação\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = dataset[\"train\"]\n",
    "val_dataset = dataset[\"test\"]\n",
    "\n",
    "print(f\"Treino: {len(train_dataset)} exemplos, Validação: {len(val_dataset)} exemplos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7939d8e-c1f1-486b-9c74-ca47a792c528",
   "metadata": {},
   "source": [
    "### Tokenizador funcionando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "967f4f3f-75af-4b87-87a6-02b3ed771045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4670fca9911d4154948237cd041c22bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75131ea3209045d0a95d412c08d77867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Carregar o tokenizador do BERTimbau\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=384,\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Ajustar os rótulos (start_positions, end_positions)\n",
    "    offset_mapping = tokenized.pop(\"offset_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # Verificar se o índice i é válido para answers\n",
    "        if i >= len(answers):\n",
    "            print(f\"Exemplo com inconsistente: {examples}\")\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "            continue\n",
    "\n",
    "        input_ids = tokenized[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # Resposta\n",
    "        answer = answers[i]\n",
    "        if not answer[\"text\"] or not answer[\"answer_start\"]:\n",
    "            print(f\"Resposta inválida em exemplo: {examples}\")\n",
    "            start_positions.append(cls_index)\n",
    "            end_positions.append(cls_index)\n",
    "            continue\n",
    "\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = start_char + len(answer[\"text\"][0])\n",
    "\n",
    "        # Identificar a posição dos tokens correspondentes\n",
    "        token_start_index, token_end_index = 0, 0\n",
    "        for idx, (start, end) in enumerate(offsets):\n",
    "            if start <= start_char < end:\n",
    "                token_start_index = idx\n",
    "            if start < end_char <= end:\n",
    "                token_end_index = idx\n",
    "\n",
    "        # Caso a resposta esteja fora do contexto\n",
    "        if start_char < offsets[0][0] or end_char > offsets[-1][1]:\n",
    "            start_positions.append(cls_index)\n",
    "            end_positions.append(cls_index)\n",
    "        else:\n",
    "            start_positions.append(token_start_index)\n",
    "            end_positions.append(token_end_index)\n",
    "\n",
    "    tokenized[\"start_positions\"] = start_positions\n",
    "    tokenized[\"end_positions\"] = end_positions\n",
    "    return tokenized            \n",
    "\n",
    "\n",
    "# Aplicar a tokenização\n",
    "tokenized_train = train_dataset.map(preprocess_function, batched=True, remove_columns=train_dataset.column_names)\n",
    "tokenized_val = val_dataset.map(preprocess_function, batched=True, remove_columns=val_dataset.column_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e7d4ca-7ce4-4888-bb62-28e6043d0038",
   "metadata": {},
   "source": [
    "### Tokenizador não funcionando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c51e161-30cf-468a-903e-38d9463d0cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(example):\n",
    "    # Tokenizar um único exemplo\n",
    "    tokenized = tokenizer(\n",
    "        example[\"question\"],\n",
    "        example[\"context\"],\n",
    "        truncation=\"longest_first\",  # Truncamento equilibrado\n",
    "        max_length=512,             # Aumentar o limite de tokens\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Ajustar os rótulos (start_positions, end_positions)\n",
    "    offset_mapping = tokenized.pop(\"offset_mapping\")\n",
    "    answer = example[\"answers\"]\n",
    "\n",
    "    # Validar e filtrar offsets inválidos\n",
    "    valid_offsets = []\n",
    "    for mapping in offset_mapping:\n",
    "        # Garantir que mapping é uma tupla/lista com exatamente dois elementos\n",
    "        if isinstance(mapping, (tuple, list)) and len(mapping) == 2:\n",
    "            valid_offsets.append(mapping)\n",
    "        else:\n",
    "            valid_offsets.append((0, 0))  # Substituir valores inválidos por (0, 0)\n",
    "\n",
    "    # Substituir o offset_mapping pelos valores validados\n",
    "    offset_mapping = valid_offsets\n",
    "\n",
    "    # Obter os índices de início e fim da resposta no contexto\n",
    "    start_char = answer[\"answer_start\"][0]\n",
    "    end_char = start_char + len(answer[\"text\"][0])\n",
    "\n",
    "    # Verificar se a resposta está dentro dos limites do contexto truncado\n",
    "    first_token_start, _ = offset_mapping[0]\n",
    "    _, last_token_end = offset_mapping[-1]\n",
    "\n",
    "    if start_char < first_token_start or end_char > last_token_end:\n",
    "        print(f\"Ignorando exemplo: Resposta fora do contexto truncado.\\nExemplo: {example}\\n\")\n",
    "        return {}\n",
    "\n",
    "    # Identificar os índices dos tokens correspondentes\n",
    "    token_start_index = 0\n",
    "    token_end_index = 0\n",
    "    for idx, mapping in enumerate(offset_mapping):\n",
    "        # Ignorar offsets inválidos como (0, 0)\n",
    "        if mapping == (0, 0):\n",
    "            continue\n",
    "\n",
    "        start, end = mapping\n",
    "        if start <= start_char < end:\n",
    "            token_start_index = idx\n",
    "        if start < end_char <= end:\n",
    "            token_end_index = idx\n",
    "\n",
    "    # Adicionar as posições calculadas\n",
    "    tokenized[\"start_positions\"] = token_start_index\n",
    "    tokenized[\"end_positions\"] = token_end_index\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "# Aplicar tokenização\n",
    "tokenized_train = train_dataset.map(preprocess_function, batched=False)\n",
    "tokenized_val = val_dataset.map(preprocess_function, batched=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "264417e6-9b6a-4a65-a566-fb7d966e7019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/leticia/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_43078/1739306116.py:23: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1746' max='1746' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1746/1746 04:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.472200</td>\n",
       "      <td>0.000299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1746, training_loss=0.1358893238955347, metrics={'train_runtime': 268.9523, 'train_samples_per_second': 51.868, 'train_steps_per_second': 6.492, 'total_flos': 2733817317350400.0, 'train_loss': 0.1358893238955347, 'epoch': 3.0})"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "\n",
    "# Carregar o modelo pré-treinado\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
    "\n",
    "# Configurar o treinamento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bertimbau-finetuned-qa\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    logging_steps=500,\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,  # Use mixed precision se sua GPU suportar\n",
    ")\n",
    "\n",
    "# Criar o Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Treinar o modelo\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "73d67a54-6bb5-4941-9e84-27907d4ceb82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./bertimbau-finetuned-qa/tokenizer_config.json',\n",
       " './bertimbau-finetuned-qa/special_tokens_map.json',\n",
       " './bertimbau-finetuned-qa/vocab.txt',\n",
       " './bertimbau-finetuned-qa/added_tokens.json',\n",
       " './bertimbau-finetuned-qa/tokenizer.json')"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./bertimbau-finetuned-qa\")\n",
    "tokenizer.save_pretrained(\"./bertimbau-finetuned-qa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "0b3fbfd2-8e4b-475a-b51a-d4ef02dfcc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "# # Caminho para o modelo treinado\n",
    "# model_path = \"caminho/para/seu/modelo\"\n",
    "\n",
    "# # Carregar o modelo e o tokenizador\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "# model = AutoModelForQuestionAnswering.from_pretrained(model_path)\n",
    "\n",
    "# Criar o pipeline para Question Answering\n",
    "qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "58d74906-c1df-4ba2-8b75-beb07a5b9c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta: .\n",
      "No entanto\n",
      "Pontuação: 6.368967311008722e-13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leticia/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/pipelines/question_answering.py:391: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Definir contexto e pergunta\n",
    "context = \"\"\"\n",
    "Inicialmente, as autoridades não conseguiram entrar em contato com a Reserva Natural Nacional de Wolong, lar de cerca de 280 pandas gigantes.\n",
    "No entanto, o Ministério das Relações Exteriores disse mais tarde que um grupo de 31 turistas britânicos que visitavam a Reserva de Pandas de Wolong\n",
    "na área atingida pelo terremoto retornaram sãos e salvos para Chengdu. No entanto, o bem-estar de um número ainda maior de pandas nas reservas\n",
    "vizinhas de pandas permaneceu desconhecido. Cinco guardas de segurança da reserva foram mortos pelo terremoto.\n",
    "\"\"\"\n",
    "question = \"Quantos seguranças morreram na reserva?\"\n",
    "\n",
    "# Fazer a pergunta\n",
    "result = qa_pipeline({\"context\": context, \"question\": question})\n",
    "\n",
    "# Exibir a resposta\n",
    "print(f\"Resposta: {result['answer']}\")\n",
    "print(f\"Pontuação: {result['score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a4e29b75-a872-4f5b-a9a7-3c2384f51ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta: .\n",
      "Pontuação: 6.054051032190755e-13\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"\n",
    "O panda desaparecido foi encontrado morto sob os escombros de um recinto.\n",
    "Mao Mao, de nove anos, mãe de cinco filhos no centro de criação, foi descoberta na segunda-feira, seu corpo esmagado por uma parede em seu recinto.\n",
    "\"\"\"\n",
    "question = \"Quantos anos tinha o panda Mao Mao?\"\n",
    "\n",
    "result = qa_pipeline({\"context\": context, \"question\": question})\n",
    "print(f\"Resposta: {result['answer']}\")\n",
    "print(f\"Pontuação: {result['score']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "55ef16ad-f82b-4d5f-8e3c-5c70ea908feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta: .\n",
      "Pontuação: 1.2422687661536869e-12\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"\n",
    "Cinco guardas de segurança da reserva foram mortos pelo terremoto.\n",
    "\"\"\"\n",
    "question = \"Quantos seguranças morreram na reserva?\"\n",
    "\n",
    "result = qa_pipeline({\"context\": context, \"question\": question})\n",
    "print(f\"Resposta: {result['answer']}\")\n",
    "print(f\"Pontuação: {result['score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d464c7e7-01b2-4eb4-bfe6-cb2ba4959430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': 'Em 2015, Beyoncé assinou uma carta aberta para a qual a Campanha ONE estava coletando assinaturas; a carta foi endereçada a Angela Merkel e Nkosazana Dlamini-Zuma, pedindo que elas se concentrassem nas mulheres enquanto atuassem como chefes do G7 na Alemanha e da UA na África do Sul, respectivamente, que começarão a definir as prioridades no financiamento do desenvolvimento antes de uma cúpula principal da ONU em setembro de 2015, que estabelecerá novas metas de desenvolvimento para a geração.',\n",
       " 'question': 'O que precisava ser definido no desenvolvimento do financiamento?',\n",
       " 'answers': {'answer_start': [313], 'text': ['prioridades']}}"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3922bdbd-1bed-4e38-b9ab-2294c5aeb02a",
   "metadata": {},
   "source": [
    "# Exemplo em português não funcionou bem. Traduzir mais exemplos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f985d6f8-4269-4cd0-8b1a-8e720f1e33d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
