{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478ccfd5-754d-499f-a287-a64c0b7e9141",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers datasets torch scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81c96b29-1cdb-43aa-85ee-d346c4882472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU dispon√≠vel: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Verifica se h√° uma GPU dispon√≠vel\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU dispon√≠vel: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"GPU n√£o dispon√≠vel, usando CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8d9c46d4-1d40-428e-843b-08a424705649",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from tqdm import tqdm  # Para barra de progresso\n",
    "from collections import Counter\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c9be9a6-ffc0-4e93-99bb-7bb63329d74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Carregar o dataset\n",
    "# Aqui estamos usando o dataset de reviews do IMDB como exemplo.\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba268ba1-102f-41b5-ae46-98296b865edb",
   "metadata": {},
   "source": [
    "## brief exploratory analysis of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f030586d-5aae-4c2d-996a-e85a61d6c048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['neg', 'pos'], id=None)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d038eaa2-f8a5-4265-8971-b5c6ab8faea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f04a9a3-e9df-4b5c-a10f-ab77629f1292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribui√ß√£o de classes: Counter({0: 12500, 1: 12500})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "labels = dataset[\"train\"][\"label\"]\n",
    "label_counts = Counter(labels)\n",
    "print(\"Distribui√ß√£o de classes:\", label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95b9eedb-5e35-41a7-aec7-848871aa6b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comprimento m√©dio: 233.79 palavras\n"
     ]
    }
   ],
   "source": [
    "text_lengths = [len(text.split()) for text in dataset[\"train\"][\"text\"]]\n",
    "print(f\"Comprimento m√©dio: {sum(text_lengths) / len(text_lengths):.2f} palavras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "079c6817-e4a8-444f-9077-944b9f511544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comprimento m√°ximo: 2470 palavras\n"
     ]
    }
   ],
   "source": [
    "max_length = max(len(text.split()) for text in dataset[\"train\"][\"text\"])\n",
    "print(f\"Comprimento m√°ximo: {max_length} palavras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f386560-520d-480a-abc6-f6e0e6afca8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Textos vazios: 0\n"
     ]
    }
   ],
   "source": [
    "empty_texts = [text for text in dataset[\"train\"][\"text\"] if len(text.strip()) == 0]\n",
    "print(f\"Textos vazios: {len(empty_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e056893-90e4-4c60-9e32-03d8d9732e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Textos inv√°lidos: 0\n"
     ]
    }
   ],
   "source": [
    "invalid_texts = [text for text in dataset[\"train\"][\"text\"] if not any(char.isalpha() for char in text)]\n",
    "print(f\"Textos inv√°lidos: {len(invalid_texts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9435d805-3c12-4a32-9aa6-59ea5860ece2",
   "metadata": {},
   "source": [
    "## Instantiating the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25b62ee4-a09d-4db4-96c2-0fc81b76b6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Carregar o tokenizer do BERT\n",
    "# O tokenizer converte texto em tokens compat√≠veis com o modelo BERT.\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa71bd9b-c892-47f9-aa77-be51a2a2870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para tokenizar os textos\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# Aplicar a tokeniza√ß√£o no dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d8928ab-dfcd-4515-82a7-d7bd3780af6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Preparar o dataset para o treinamento\n",
    "# O Hugging Face exige colunas espec√≠ficas: 'input_ids' e 'attention_mask'.\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])  # Remove o texto original\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")  # Renomeia a coluna alvo\n",
    "tokenized_datasets.set_format(\"torch\")  # Converte para tensores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b8f718e-a48f-4003-a210-e62a3c06d6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir em treino e valida√ß√£o\n",
    "train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(2000))  # Exemplo reduzido\n",
    "test_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(500))    # Exemplo reduzido\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f113be60-199f-4e5e-996e-44a41842f8d4",
   "metadata": {},
   "source": [
    "## Instantiating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89732f07-8027-4fba-8731-e6ee4f003df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 4. Carregar o modelo pr√©-treinado\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3452136e-587e-4cd1-bdf2-ed81a69fa9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Definir m√©tricas para avalia√ß√£o\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"binary\")\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c66cf9d-f568-48f0-ac71-c1d06e6338e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leticia/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 6. Configurar o treinamento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954b8587-a00a-4bbe-8689-d683fb49814c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install accelerate>=0.26.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bde93f57-0b4a-45dc-9d07-8f01c799fecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c9bb8a9-f8c7-44fe-ae56-e6ec27144faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 03:36, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.327600</td>\n",
       "      <td>0.359210</td>\n",
       "      <td>0.878000</td>\n",
       "      <td>0.883365</td>\n",
       "      <td>0.833935</td>\n",
       "      <td>0.939024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.251000</td>\n",
       "      <td>0.379327</td>\n",
       "      <td>0.904000</td>\n",
       "      <td>0.899582</td>\n",
       "      <td>0.926724</td>\n",
       "      <td>0.873984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=0.2963378931283951, metrics={'train_runtime': 217.1611, 'train_samples_per_second': 18.42, 'train_steps_per_second': 2.302, 'total_flos': 1052444221440000.0, 'train_loss': 0.2963378931283951, 'epoch': 2.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. Treinar o modelo\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93f0b8c3-23a3-4110-972d-1dbc88e0876f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3793271481990814, 'eval_accuracy': 0.904, 'eval_f1': 0.899581589958159, 'eval_precision': 0.9267241379310345, 'eval_recall': 0.8739837398373984, 'eval_runtime': 7.0195, 'eval_samples_per_second': 71.231, 'eval_steps_per_second': 8.975, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "# 8. Avaliar o modelo\n",
    "metrics = trainer.evaluate()\n",
    "print(metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a0002f2-4f0e-45f3-9ce7-e18477af2c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Certificar-se de que o modelo est√° na GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb43408-5d63-4894-a834-c42785aacd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Fazer previs√µes com novos textos\n",
    "test_texts = [\"This movie was amazing!\", \"I hated this movie.\"]\n",
    "test_encodings = tokenizer(test_texts, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "output = model(**test_encodings)\n",
    "predictions = output.logits.argmax(dim=1)\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "faa0bc6e-1d85-424d-ac5b-e86856e1723c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo no dispositivo: cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Modelo no dispositivo: {next(model.parameters()).device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "274460e2-5271-40d6-9fad-ecbedb95afdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar entradas\n",
    "test_texts = [\"This movie was amazing!\", \"I hated this movie.\", \"I don't think the movie is really good\"]\n",
    "inputs = tokenizer(test_texts, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "# Mover os inputs para o mesmo dispositivo do modelo\n",
    "inputs = {key: value.to(device) for key, value in inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6fc46de5-ef7d-4ccc-a913-e3ecff19e2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Fazer a infer√™ncia\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Obter as previs√µes\n",
    "predictions = outputs.logits.argmax(dim=1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28507eab-1bf6-4ff5-8557-a1bec0c4e4c7",
   "metadata": {},
   "source": [
    "## Question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "28eaa609-34cb-434e-b68b-52da88541022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b425931c5f4a4ce589b0ec71c63a387b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/8.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3db8f5e2159b493f8b9f78df3200a279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/16.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e206786b882b4a0286089c760ef7b347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/1.35M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb1d6c2b2e924c2f8d6b12fe7c87b50e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/130319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14c928d6a8de4fc484c882c0cd57d955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/11873 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '56be85543aeaaa14008c9063', 'title': 'Beyonc√©', 'context': 'Beyonc√© Giselle Knowles-Carter (/biÀêÀàj…ínse…™/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyonc√©\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".', 'question': 'When did Beyonce start becoming popular?', 'answers': {'text': ['in the late 1990s'], 'answer_start': [269]}}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Carregar dataset\n",
    "squad = load_dataset(\"squad_v2\")\n",
    "\n",
    "# Visualizar exemplos\n",
    "print(squad[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34214bcd-0563-4460-a945-57780f23f288",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    inputs = tokenizer(\n",
    "        examples[\"question\"], examples[\"context\"], truncation=True, padding=True, max_length=512\n",
    "    )\n",
    "    return inputs\n",
    "\n",
    "tokenized_squad = squad.map(preprocess_data, batched=True)\n",
    "\n",
    "# Configurar o treinamento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\", evaluation_strategy=\"epoch\", learning_rate=2e-5, num_train_epochs=3\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_squad[\"train\"],\n",
    "    eval_dataset=tokenized_squad[\"validation\"],\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e335af0f-6b92-4335-9c59-db3e176ed007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07a456a2b9954faf8f7a7d412f0db326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/451 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0438786c68e40f9bbc4fc0078ee0451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/265M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77d35e857349479092fef9e4156a4461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9f0946292f14e3f98edbf840881dfe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1b3e1c96de54b6b85bee60cf8195a9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "de6dd3cc-d260-42a9-9ba5-1fb4736014e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.14506877958774567, 'start': 10, 'end': 29, 'answer': 'modelo de linguagem'}\n"
     ]
    }
   ],
   "source": [
    "question = \"O que √© BERT?\"\n",
    "context = \"BERT √© um modelo de linguagem desenvolvido pela Google, usado para tarefas de NLP.\"\n",
    "\n",
    "result = qa_pipeline(question=question, context=context)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef10427-74c4-4d9e-8fee-a148f29937a6",
   "metadata": {},
   "source": [
    "### at√© aqui funcionando"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f443d2-5f88-4a14-9fd3-4fcf6987cd1f",
   "metadata": {},
   "source": [
    "## Bertimbau com question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca0d5d9-b114-41b4-a25b-670a376cf0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2aec08-a012-424c-a570-2a99ba7b9f22",
   "metadata": {},
   "source": [
    "### traduzindo dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b1e26b7d-b3b8-49f0-82e4-801f18b3c3b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08b3aeaf3ece4a799e5d6ebb05147fa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4528005a9424f54b4556644b9d92c7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "587385ffbd6e4df3bffa0ceee985f918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/1.82M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88cf74de235d4394b7528808734018d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb8a82d8c1a4e2cb9e124a3798c32a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c19f664b-af76-4c70-8f2e-def43f6bc9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = dataset[\"train\"].select(range(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3861d6f3-7f19-4ec0-83cf-9b7b52b4ee0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '5733be284776f41900661182', 'title': 'University_of_Notre_Dame', 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.', 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?', 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}\n"
     ]
    }
   ],
   "source": [
    "print(subset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e2b7788d-77f2-48c0-bdd8-55a54e2ebaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "title\n",
      "context\n",
      "question\n",
      "answers\n"
     ]
    }
   ],
   "source": [
    "dataset[\"train\"]\n",
    "\n",
    "for example in dataset[\"train\"][0:10]:\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211e729e-b9f3-48b0-b6a4-a412e5659194",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install deep_translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8152b332-44c8-4283-91fe-df85afc23c81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10570"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ee7cc170-4f3f-402a-8dd9-e1a64e1aec0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset = dataset[\"train\"].shuffle(seed=42).select(range(int(0.3 * len(dataset[\"train\"]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d87666b-9546-4e8d-a03c-d7a76e2c7f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from deep_translator import GoogleTranslator\n",
    "import numpy as np\n",
    "\n",
    "# Definir a fun√ß√£o ensure_serializable antes de us√°-la\n",
    "def ensure_serializable(obj):\n",
    "    \"\"\"\n",
    "    Converte qualquer valor n√£o serializ√°vel para tipos compat√≠veis com JSON.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()  # Converter ndarray para lista\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: ensure_serializable(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [ensure_serializable(value) for value in obj]\n",
    "    else:\n",
    "        return obj  # Retornar o valor original se j√° for serializ√°vel\n",
    "\n",
    "# Fun√ß√£o para traduzir texto com pausa\n",
    "def translate_text_with_pause(text, source=\"en\", target=\"pt\"):\n",
    "    time.sleep(1)  # Espera de 1 segundo entre as requisi√ß√µes\n",
    "    return GoogleTranslator(source=source, target=target).translate(text)\n",
    "\n",
    "# Traduzir o dataset com salvamento incremental\n",
    "def translate_dataset(dataset, output_file=\"translated_dataset.json\"):\n",
    "    translated_examples = []\n",
    "\n",
    "    # Carregar tradu√ß√µes parciais, se o arquivo j√° existir\n",
    "    try:\n",
    "        with open(output_file, \"r\") as file:\n",
    "            content = file.read().strip()\n",
    "            if content:\n",
    "                translated_examples = json.loads(content)\n",
    "                print(f\"Carregadas {len(translated_examples)} tradu√ß√µes existentes.\")\n",
    "            else:\n",
    "                print(\"Arquivo vazio encontrado. Come√ßando do zero.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Arquivo de sa√≠da n√£o encontrado. Criando um novo.\")\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump([], file)\n",
    "\n",
    "    # Verificar progresso\n",
    "    already_translated = len(translated_examples)\n",
    "    dataset = dataset[already_translated:]  # Ignorar exemplos j√° traduzidos\n",
    "    print(f\"Traduzindo {len(dataset)} exemplos restantes.\")\n",
    "\n",
    "    # Barra de progresso\n",
    "    for i, example in enumerate(tqdm(dataset, desc=\"Traduzindo\", unit=\"exemplo\")):\n",
    "        try:\n",
    "            # Traduzir contexto e pergunta\n",
    "            context_translated = translate_text_with_pause(example[\"context\"])\n",
    "            question_translated = translate_text_with_pause(example[\"question\"])\n",
    "\n",
    "            # Traduzir todas as respostas\n",
    "            answers_translated = {\n",
    "                \"text\": [translate_text_with_pause(ans) for ans in example[\"answers\"][\"text\"]],\n",
    "                \"answer_start\": [int(start) for start in example[\"answers\"][\"answer_start\"]],  # Converter para int\n",
    "            }\n",
    "\n",
    "            # Adicionar exemplo traduzido\n",
    "            translated_example = {\n",
    "                \"context\": context_translated,\n",
    "                \"question\": question_translated,\n",
    "                \"answers\": answers_translated,\n",
    "            }\n",
    "\n",
    "            # Garantir que todos os valores s√£o serializ√°veis\n",
    "            translated_example = ensure_serializable(translated_example)\n",
    "            translated_examples.append(translated_example)\n",
    "\n",
    "            # Salvar progresso no arquivo JSON\n",
    "            with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "                json.dump(translated_examples, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao traduzir exemplo {i}: {e}\")\n",
    "            continue  # Pula para o pr√≥ximo exemplo\n",
    "\n",
    "    print(f\"Tradu√ß√£o conclu√≠da. Total traduzido: {len(translated_examples)} exemplos.\")\n",
    "    return translated_examples\n",
    "\n",
    "# Exemplo de uso\n",
    "# Criar subconjunto de treino\n",
    "train_subset = dataset[\"train\"].shuffle(seed=42).select(range(int(0.3 * len(dataset[\"train\"]))))\n",
    "train_subset = train_subset.to_pandas().to_dict(orient=\"records\")\n",
    "print(\"Total de entradas do dataset: \", len(train_subset))\n",
    "\n",
    "# Traduzir o conjunto de treino com barra de progresso\n",
    "translated_train = translate_dataset(train_subset, \"translated_train.json\")\n",
    "\n",
    "\n",
    "val_subset = dataset[\"validation\"].shuffle(seed=42).select(range(int(0.3 * len(dataset[\"validation\"]))))\n",
    "val_subset = val_subset.to_pandas().to_dict(orient=\"records\")\n",
    "# Traduzir o conjunto de valida√ß√£o\n",
    "translated_validation = translate_dataset(val_subset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8031c1-fdb6-4f24-9103-182b17e7f68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from deep_translator import GoogleTranslator\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Fun√ß√£o para garantir que os valores s√£o serializ√°veis\n",
    "def ensure_serializable(obj):\n",
    "    \"\"\"\n",
    "    Converte valores incompat√≠veis com JSON, como ndarrays, para tipos serializ√°veis.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, np.ndarray):  # Converte ndarray para lista\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, dict):  # Converte dicion√°rios recursivamente\n",
    "        return {key: ensure_serializable(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):  # Converte listas recursivamente\n",
    "        return [ensure_serializable(value) for value in obj]\n",
    "    else:\n",
    "        return obj  # Retorna o valor original se j√° for serializ√°vel\n",
    "\n",
    "# Fun√ß√£o para traduzir texto\n",
    "def translate_text(text, source=\"en\", target=\"pt\"):\n",
    "    try:\n",
    "        return GoogleTranslator(source=source, target=target).translate(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao traduzir texto: {text}\\n{e}\")\n",
    "        return text  # Retorna o texto original em caso de falha\n",
    "\n",
    "# Fun√ß√£o para tradu√ß√£o paralela de um conjunto de textos\n",
    "def parallel_translate(texts, max_workers=4):\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        translations = list(tqdm(executor.map(translate_text, texts), total=len(texts), desc=\"Traduzindo textos\"))\n",
    "    return translations\n",
    "\n",
    "# Fun√ß√£o para traduzir o dataset\n",
    "def translate_dataset(dataset, output_file=\"translated_dataset.json\", max_workers=4):\n",
    "    translated_examples = []\n",
    "\n",
    "    # Carregar tradu√ß√µes parciais de forma robusta\n",
    "    try:\n",
    "        with open(output_file, \"r\", encoding=\"utf-8\") as file:\n",
    "            content = file.read().strip()  # Remover espa√ßos em branco\n",
    "            if content:  # Verifica se o arquivo n√£o est√° vazio\n",
    "                translated_examples = json.loads(content)\n",
    "                print(f\"Carregadas {len(translated_examples)} tradu√ß√µes existentes.\")\n",
    "            else:\n",
    "                print(\"Arquivo vazio encontrado. Come√ßando do zero.\")\n",
    "                translated_examples = []\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        print(\"Arquivo inexistente ou corrompido. Come√ßando do zero.\")\n",
    "        translated_examples = []\n",
    "\n",
    "    # Verificar progresso\n",
    "    already_translated = len(translated_examples)\n",
    "    dataset = dataset[already_translated:]  # Ignorar exemplos j√° traduzidos\n",
    "    print(f\"Traduzindo {len(dataset)} exemplos restantes.\")\n",
    "\n",
    "    # Preparar os campos para tradu√ß√£o paralela\n",
    "    contexts = [example[\"context\"] for example in dataset]\n",
    "    questions = [example[\"question\"] for example in dataset]\n",
    "    answers = [example[\"answers\"][\"text\"] for example in dataset]\n",
    "\n",
    "    # Traduzir paralelamente os campos\n",
    "    print(\"Traduzindo contextos...\")\n",
    "    translated_contexts = parallel_translate(contexts, max_workers=max_workers)\n",
    "\n",
    "    print(\"Traduzindo perguntas...\")\n",
    "    translated_questions = parallel_translate(questions, max_workers=max_workers)\n",
    "\n",
    "    print(\"Traduzindo respostas...\")\n",
    "    translated_answers = [parallel_translate(answer, max_workers=max_workers) for answer in answers]\n",
    "\n",
    "    # Reconstruir os exemplos traduzidos\n",
    "    for i in range(len(dataset)):\n",
    "        translated_example = {\n",
    "            \"context\": translated_contexts[i],\n",
    "            \"question\": translated_questions[i],\n",
    "            \"answers\": {\n",
    "                \"text\": translated_answers[i],\n",
    "                \"answer_start\": ensure_serializable(dataset[i][\"answers\"][\"answer_start\"]),  # Serializ√°vel\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # Garantir que todo o exemplo √© serializ√°vel\n",
    "        translated_example = ensure_serializable(translated_example)\n",
    "        translated_examples.append(translated_example)\n",
    "\n",
    "        # Salvamento incremental\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(translated_examples, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Tradu√ß√£o conclu√≠da. Total traduzido: {len(translated_examples)} exemplos.\")\n",
    "    return translated_examples\n",
    "\n",
    "# Carregar o SQuAD\n",
    "dataset = load_dataset(\"squad\")\n",
    "\n",
    "# Selecionar um subconjunto (opcional)\n",
    "train_subset = dataset[\"train\"].shuffle(seed=42).select(range(500))  # Traduzir 500 exemplos\n",
    "train_subset = train_subset.to_pandas().to_dict(orient=\"records\")\n",
    "\n",
    "# Traduzir o conjunto de treino com paralelismo\n",
    "translated_train = translate_dataset(train_subset, output_file=\"translated_train.json\", max_workers=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed586e5d-1abd-4b41-a789-7cc41c5e05e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from deep_translator import GoogleTranslator\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Fun√ß√£o para garantir que os valores s√£o serializ√°veis\n",
    "def ensure_serializable(obj):\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: ensure_serializable(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [ensure_serializable(value) for value in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Salvamento seguro\n",
    "def safe_save_json(data, output_file):\n",
    "    temp_file = output_file + \".tmp\"\n",
    "    with open(temp_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "    os.replace(temp_file, output_file)\n",
    "\n",
    "# Fun√ß√£o para traduzir texto\n",
    "def translate_text(text, source=\"en\", target=\"pt\"):\n",
    "    try:\n",
    "        return GoogleTranslator(source=source, target=target).translate(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao traduzir texto: {text}\\n{e}\")\n",
    "        return text\n",
    "\n",
    "# Tradu√ß√£o paralela\n",
    "def parallel_translate(texts, max_workers=4):\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        translations = list(tqdm(executor.map(translate_text, texts), total=len(texts), desc=\"Traduzindo textos\"))\n",
    "    return translations\n",
    "\n",
    "# Fun√ß√£o principal de tradu√ß√£o\n",
    "def translate_dataset_in_batches(dataset, output_file=\"translated_dataset.json\", max_workers=4, batch_size=500, max_samples=5000):\n",
    "    # Carregar tradu√ß√µes parciais\n",
    "    try:\n",
    "        with open(output_file, \"r\", encoding=\"utf-8\") as file:\n",
    "            content = file.read().strip()  # Verifica se o arquivo est√° vazio\n",
    "            if content:\n",
    "                translated_examples = json.loads(content)\n",
    "                print(f\"Carregadas {len(translated_examples)} tradu√ß√µes existentes.\")\n",
    "            else:\n",
    "                print(\"Arquivo vazio encontrado. Come√ßando do zero.\")\n",
    "                translated_examples = []\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        print(\"Arquivo inexistente ou corrompido. Come√ßando do zero.\")\n",
    "        translated_examples = []\n",
    "\n",
    "    # Determinar quantos exemplos j√° foram traduzidos\n",
    "    already_translated = len(translated_examples)\n",
    "    if already_translated >= max_samples:\n",
    "        print(f\"J√° foram traduzidos {already_translated} exemplos. Nenhuma tradu√ß√£o adicional necess√°ria.\")\n",
    "        return translated_examples\n",
    "\n",
    "    # Traduzir em lotes\n",
    "    for start in range(already_translated, min(max_samples, len(dataset)), batch_size):\n",
    "        end = min(start + batch_size, max_samples)\n",
    "        print(f\"Traduzindo exemplos {start} a {end}...\")\n",
    "\n",
    "        # Preparar lote atual\n",
    "        batch = dataset[start:end]\n",
    "        contexts = [example[\"context\"] for example in batch]\n",
    "        questions = [example[\"question\"] for example in batch]\n",
    "        answers = [example[\"answers\"][\"text\"] for example in batch]\n",
    "\n",
    "        # Traduzir os campos\n",
    "        translated_contexts = parallel_translate(contexts, max_workers=max_workers)\n",
    "        translated_questions = parallel_translate(questions, max_workers=max_workers)\n",
    "        translated_answers = [parallel_translate(answer, max_workers=max_workers) for answer in answers]\n",
    "\n",
    "        # Reconstruir exemplos traduzidos\n",
    "        for i in range(len(batch)):\n",
    "            translated_example = {\n",
    "                \"context\": translated_contexts[i],\n",
    "                \"question\": translated_questions[i],\n",
    "                \"answers\": {\n",
    "                    \"text\": translated_answers[i],\n",
    "                    \"answer_start\": ensure_serializable(batch[i][\"answers\"][\"answer_start\"]),\n",
    "                },\n",
    "            }\n",
    "            translated_example = ensure_serializable(translated_example)\n",
    "            translated_examples.append(translated_example)\n",
    "\n",
    "        # Salvamento incremental\n",
    "        safe_save_json(translated_examples, output_file)\n",
    "\n",
    "        print(f\"Tradu√ß√£o de {end} exemplos conclu√≠da. Total traduzido at√© agora: {len(translated_examples)} exemplos.\")\n",
    "\n",
    "        # Verifica se o limite foi atingido\n",
    "        if len(translated_examples) >= max_samples:\n",
    "            break\n",
    "\n",
    "    print(f\"Tradu√ß√£o conclu√≠da. Total traduzido: {len(translated_examples)} exemplos.\")\n",
    "    return translated_examples\n",
    "\n",
    "# Carregar o SQuAD\n",
    "dataset = load_dataset(\"squad\")[\"train\"].to_pandas().to_dict(orient=\"records\")  # Carregar como lista de dicion√°rios\n",
    "\n",
    "# Traduzir em lotes at√© atingir 5.000 exemplos\n",
    "translated_train = translate_dataset_in_batches(\n",
    "    dataset,\n",
    "    output_file=\"translated_train.json\",\n",
    "    max_workers=8,\n",
    "    batch_size=500,\n",
    "    max_samples=5000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "69701d34-3ec1-470e-95bd-c9cfb652f771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'=0.26.0'\t    huggingread.txt\t     LLM\n",
      " api_key.txt\t    hugging.txt\t\t     results\n",
      " bert.ipynb\t    langchain_test.ipynb     translated_train.json\n",
      " first_test.ipynb   langchain_to_git.ipynb\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "976970ef-30c0-49f4-a22f-736acaeca5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context': 'O Pew Forum on Religion & Public Life classifica o Egito como o quinto pior pa√≠s do mundo em liberdade religiosa. A Comiss√£o dos Estados Unidos sobre Liberdade Religiosa Internacional, uma ag√™ncia independente bipartid√°ria do governo dos EUA, colocou o Egito em sua lista de observa√ß√£o de pa√≠ses que exigem monitoramento rigoroso devido √† natureza e extens√£o das viola√ß√µes da liberdade religiosa praticadas ou toleradas pelo governo. De acordo com uma pesquisa Pew Global Attitudes de 2010, 84% dos eg√≠pcios entrevistados apoiaram a pena de morte para aqueles que abandonam o islamismo; 77% apoiaram chicotadas e cortes de m√£os por roubo e furto; e 82% apoiam o apedrejamento de uma pessoa que comete adult√©rio.', 'question': 'Qual a porcentagem de eg√≠pcios entrevistados que apoiam a pena de morte para aqueles que abandonam o islamismo?', 'answers': {'text': ['84%'], 'answer_start': [468]}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Carregar o dataset traduzido\n",
    "with open(\"translated_train.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Exemplo de estrutura de um dado\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "912dfcb5-cf44-46bb-b86f-ab1846fa4229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino: 4500 exemplos, Valida√ß√£o: 500 exemplos\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Converter para o formato do datasets\n",
    "dataset = Dataset.from_dict({\n",
    "    \"context\": [example[\"context\"] for example in data],\n",
    "    \"question\": [example[\"question\"] for example in data],\n",
    "    \"answers\": [example[\"answers\"] for example in data],\n",
    "})\n",
    "\n",
    "# Dividir em treino e valida√ß√£o\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = dataset[\"train\"]\n",
    "val_dataset = dataset[\"test\"]\n",
    "\n",
    "print(f\"Treino: {len(train_dataset)} exemplos, Valida√ß√£o: {len(val_dataset)} exemplos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7939d8e-c1f1-486b-9c74-ca47a792c528",
   "metadata": {},
   "source": [
    "### Tokenizador funcionando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "967f4f3f-75af-4b87-87a6-02b3ed771045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4670fca9911d4154948237cd041c22bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75131ea3209045d0a95d412c08d77867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Carregar o tokenizador do BERTimbau\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=384,\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Ajustar os r√≥tulos (start_positions, end_positions)\n",
    "    offset_mapping = tokenized.pop(\"offset_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # Verificar se o √≠ndice i √© v√°lido para answers\n",
    "        if i >= len(answers):\n",
    "            print(f\"Exemplo com inconsistente: {examples}\")\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "            continue\n",
    "\n",
    "        input_ids = tokenized[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # Resposta\n",
    "        answer = answers[i]\n",
    "        if not answer[\"text\"] or not answer[\"answer_start\"]:\n",
    "            print(f\"Resposta inv√°lida em exemplo: {examples}\")\n",
    "            start_positions.append(cls_index)\n",
    "            end_positions.append(cls_index)\n",
    "            continue\n",
    "\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = start_char + len(answer[\"text\"][0])\n",
    "\n",
    "        # Identificar a posi√ß√£o dos tokens correspondentes\n",
    "        token_start_index, token_end_index = 0, 0\n",
    "        for idx, (start, end) in enumerate(offsets):\n",
    "            if start <= start_char < end:\n",
    "                token_start_index = idx\n",
    "            if start < end_char <= end:\n",
    "                token_end_index = idx\n",
    "\n",
    "        # Caso a resposta esteja fora do contexto\n",
    "        if start_char < offsets[0][0] or end_char > offsets[-1][1]:\n",
    "            start_positions.append(cls_index)\n",
    "            end_positions.append(cls_index)\n",
    "        else:\n",
    "            start_positions.append(token_start_index)\n",
    "            end_positions.append(token_end_index)\n",
    "\n",
    "    tokenized[\"start_positions\"] = start_positions\n",
    "    tokenized[\"end_positions\"] = end_positions\n",
    "    return tokenized            \n",
    "\n",
    "\n",
    "# Aplicar a tokeniza√ß√£o\n",
    "tokenized_train = train_dataset.map(preprocess_function, batched=True, remove_columns=train_dataset.column_names)\n",
    "tokenized_val = val_dataset.map(preprocess_function, batched=True, remove_columns=val_dataset.column_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e7d4ca-7ce4-4888-bb62-28e6043d0038",
   "metadata": {},
   "source": [
    "### Tokenizador n√£o funcionando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c51e161-30cf-468a-903e-38d9463d0cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(example):\n",
    "    # Tokenizar um √∫nico exemplo\n",
    "    tokenized = tokenizer(\n",
    "        example[\"question\"],\n",
    "        example[\"context\"],\n",
    "        truncation=\"longest_first\",  # Truncamento equilibrado\n",
    "        max_length=512,             # Aumentar o limite de tokens\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Ajustar os r√≥tulos (start_positions, end_positions)\n",
    "    offset_mapping = tokenized.pop(\"offset_mapping\")\n",
    "    answer = example[\"answers\"]\n",
    "\n",
    "    # Validar e filtrar offsets inv√°lidos\n",
    "    valid_offsets = []\n",
    "    for mapping in offset_mapping:\n",
    "        # Garantir que mapping √© uma tupla/lista com exatamente dois elementos\n",
    "        if isinstance(mapping, (tuple, list)) and len(mapping) == 2:\n",
    "            valid_offsets.append(mapping)\n",
    "        else:\n",
    "            valid_offsets.append((0, 0))  # Substituir valores inv√°lidos por (0, 0)\n",
    "\n",
    "    # Substituir o offset_mapping pelos valores validados\n",
    "    offset_mapping = valid_offsets\n",
    "\n",
    "    # Obter os √≠ndices de in√≠cio e fim da resposta no contexto\n",
    "    start_char = answer[\"answer_start\"][0]\n",
    "    end_char = start_char + len(answer[\"text\"][0])\n",
    "\n",
    "    # Verificar se a resposta est√° dentro dos limites do contexto truncado\n",
    "    first_token_start, _ = offset_mapping[0]\n",
    "    _, last_token_end = offset_mapping[-1]\n",
    "\n",
    "    if start_char < first_token_start or end_char > last_token_end:\n",
    "        print(f\"Ignorando exemplo: Resposta fora do contexto truncado.\\nExemplo: {example}\\n\")\n",
    "        return {}\n",
    "\n",
    "    # Identificar os √≠ndices dos tokens correspondentes\n",
    "    token_start_index = 0\n",
    "    token_end_index = 0\n",
    "    for idx, mapping in enumerate(offset_mapping):\n",
    "        # Ignorar offsets inv√°lidos como (0, 0)\n",
    "        if mapping == (0, 0):\n",
    "            continue\n",
    "\n",
    "        start, end = mapping\n",
    "        if start <= start_char < end:\n",
    "            token_start_index = idx\n",
    "        if start < end_char <= end:\n",
    "            token_end_index = idx\n",
    "\n",
    "    # Adicionar as posi√ß√µes calculadas\n",
    "    tokenized[\"start_positions\"] = token_start_index\n",
    "    tokenized[\"end_positions\"] = token_end_index\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "# Aplicar tokeniza√ß√£o\n",
    "tokenized_train = train_dataset.map(preprocess_function, batched=False)\n",
    "tokenized_val = val_dataset.map(preprocess_function, batched=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "264417e6-9b6a-4a65-a566-fb7d966e7019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/leticia/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_43078/1739306116.py:23: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1746' max='1746' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1746/1746 04:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.472200</td>\n",
       "      <td>0.000299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1746, training_loss=0.1358893238955347, metrics={'train_runtime': 268.9523, 'train_samples_per_second': 51.868, 'train_steps_per_second': 6.492, 'total_flos': 2733817317350400.0, 'train_loss': 0.1358893238955347, 'epoch': 3.0})"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "\n",
    "# Carregar o modelo pr√©-treinado\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
    "\n",
    "# Configurar o treinamento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bertimbau-finetuned-qa\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    logging_steps=500,\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,  # Use mixed precision se sua GPU suportar\n",
    ")\n",
    "\n",
    "# Criar o Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Treinar o modelo\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "73d67a54-6bb5-4941-9e84-27907d4ceb82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./bertimbau-finetuned-qa/tokenizer_config.json',\n",
       " './bertimbau-finetuned-qa/special_tokens_map.json',\n",
       " './bertimbau-finetuned-qa/vocab.txt',\n",
       " './bertimbau-finetuned-qa/added_tokens.json',\n",
       " './bertimbau-finetuned-qa/tokenizer.json')"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./bertimbau-finetuned-qa\")\n",
    "tokenizer.save_pretrained(\"./bertimbau-finetuned-qa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "0b3fbfd2-8e4b-475a-b51a-d4ef02dfcc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "# # Caminho para o modelo treinado\n",
    "# model_path = \"caminho/para/seu/modelo\"\n",
    "\n",
    "# # Carregar o modelo e o tokenizador\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "# model = AutoModelForQuestionAnswering.from_pretrained(model_path)\n",
    "\n",
    "# Criar o pipeline para Question Answering\n",
    "qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "58d74906-c1df-4ba2-8b75-beb07a5b9c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta: .\n",
      "No entanto\n",
      "Pontua√ß√£o: 6.368967311008722e-13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leticia/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/pipelines/question_answering.py:391: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Definir contexto e pergunta\n",
    "context = \"\"\"\n",
    "Inicialmente, as autoridades n√£o conseguiram entrar em contato com a Reserva Natural Nacional de Wolong, lar de cerca de 280 pandas gigantes.\n",
    "No entanto, o Minist√©rio das Rela√ß√µes Exteriores disse mais tarde que um grupo de 31 turistas brit√¢nicos que visitavam a Reserva de Pandas de Wolong\n",
    "na √°rea atingida pelo terremoto retornaram s√£os e salvos para Chengdu. No entanto, o bem-estar de um n√∫mero ainda maior de pandas nas reservas\n",
    "vizinhas de pandas permaneceu desconhecido. Cinco guardas de seguran√ßa da reserva foram mortos pelo terremoto.\n",
    "\"\"\"\n",
    "question = \"Quantos seguran√ßas morreram na reserva?\"\n",
    "\n",
    "# Fazer a pergunta\n",
    "result = qa_pipeline({\"context\": context, \"question\": question})\n",
    "\n",
    "# Exibir a resposta\n",
    "print(f\"Resposta: {result['answer']}\")\n",
    "print(f\"Pontua√ß√£o: {result['score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a4e29b75-a872-4f5b-a9a7-3c2384f51ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta: .\n",
      "Pontua√ß√£o: 6.054051032190755e-13\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"\n",
    "O panda desaparecido foi encontrado morto sob os escombros de um recinto.\n",
    "Mao Mao, de nove anos, m√£e de cinco filhos no centro de cria√ß√£o, foi descoberta na segunda-feira, seu corpo esmagado por uma parede em seu recinto.\n",
    "\"\"\"\n",
    "question = \"Quantos anos tinha o panda Mao Mao?\"\n",
    "\n",
    "result = qa_pipeline({\"context\": context, \"question\": question})\n",
    "print(f\"Resposta: {result['answer']}\")\n",
    "print(f\"Pontua√ß√£o: {result['score']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "55ef16ad-f82b-4d5f-8e3c-5c70ea908feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta: .\n",
      "Pontua√ß√£o: 1.2422687661536869e-12\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"\n",
    "Cinco guardas de seguran√ßa da reserva foram mortos pelo terremoto.\n",
    "\"\"\"\n",
    "question = \"Quantos seguran√ßas morreram na reserva?\"\n",
    "\n",
    "result = qa_pipeline({\"context\": context, \"question\": question})\n",
    "print(f\"Resposta: {result['answer']}\")\n",
    "print(f\"Pontua√ß√£o: {result['score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d464c7e7-01b2-4eb4-bfe6-cb2ba4959430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': 'Em 2015, Beyonc√© assinou uma carta aberta para a qual a Campanha ONE estava coletando assinaturas; a carta foi endere√ßada a Angela Merkel e Nkosazana Dlamini-Zuma, pedindo que elas se concentrassem nas mulheres enquanto atuassem como chefes do G7 na Alemanha e da UA na √Åfrica do Sul, respectivamente, que come√ßar√£o a definir as prioridades no financiamento do desenvolvimento antes de uma c√∫pula principal da ONU em setembro de 2015, que estabelecer√° novas metas de desenvolvimento para a gera√ß√£o.',\n",
       " 'question': 'O que precisava ser definido no desenvolvimento do financiamento?',\n",
       " 'answers': {'answer_start': [313], 'text': ['prioridades']}}"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3922bdbd-1bed-4e38-b9ab-2294c5aeb02a",
   "metadata": {},
   "source": [
    "# Exemplo em portugu√™s n√£o funcionou bem. Traduzir mais exemplos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f985d6f8-4269-4cd0-8b1a-8e720f1e33d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
