{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9fdea6c-c15f-4065-8b25-d1fcbba7c6d0",
   "metadata": {},
   "source": [
    "### Esse notebook contém testes introdutórios com LLMs utilizando os frameworks do langchain e bibliotecas como o hugging face. Contempla testes utilizando os modelos localmnte e via API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dd3511-97eb-4aaf-a220-2696308faa86",
   "metadata": {},
   "source": [
    "Utilizando langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d81dad-c9a5-4e32-a250-9ba8438366c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "\n",
    "# Configuração do modelo LLM (usando a API da OpenAI)\n",
    "\n",
    "api_key = \"minha-chave\"\n",
    "llm = OpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\",# Modelo GPT-3.5 ou GPT-4\n",
    "    openai_api_key = api_key,\n",
    "    temperature=0.7,\n",
    "    )               # Controla a criatividade\n",
    "\n",
    "# Definir um template de prompt\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"pergunta\"],  # Variável que será preenchida\n",
    "    template=\"Você é um assistente muito útil. Responda à pergunta: {pergunta}\"\n",
    ")\n",
    "\n",
    "# Criar uma cadeia (chain) com o LLM e o prompt\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# Fazer uma pergunta e obter a resposta\n",
    "pergunta = \"Qual é a capital da França?\"\n",
    "resposta = chain.run(pergunta)\n",
    "\n",
    "print(f\"Pergunta: {pergunta}\")\n",
    "print(f\"Resposta: {resposta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356a73af-9590-4479-8143-57473cdd5154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Verifique se a variável de ambiente foi carregada\n",
    "print(\"OPENAI_API_KEY da variável de ambiente:\", os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8837ebee-9da6-4270-98d0-959ac5b598c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539fc805-5d9d-4c65-a775-f6b2ac76ab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = \"minha-chave\"\n",
    "\n",
    "# Solicitação para o modelo GPT\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",  # Ou \"gpt-4\" para o modelo mais avançado\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Você é um assistente útil.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Qual é a capital da França?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Extraindo a resposta\n",
    "print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92482b6f-91bc-450b-807e-900932028cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai==0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5884746f-96b1-4ce1-af5b-3d3dc2871b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Configurar a chave diretamente\n",
    "openai_api_key = \"minha-chave\"\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.7,\n",
    "    openai_api_key=openai_api_key\n",
    ")\n",
    "\n",
    "# Depuração\n",
    "assert llm.openai_api_key == openai_api_key, \"A chave de API não foi configurada corretamente!\"\n",
    "print(\"A chave de API foi configurada corretamente no LLM.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bb8c27-7ab8-4696-bf69-04c15da7eafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade langchain openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e8350a-a730-411c-9baf-c1583a771aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Passando a chave de API diretamente\n",
    "openai_api_key = \"minha-chave\"  # Substitua pela sua chave de API\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",  # Ou \"gpt-4\"\n",
    "    temperature=0.7,\n",
    "    openai_api_key=openai_api_key  # Passando explicitamente a chave de API\n",
    ")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"Você é um assistente muito útil. Responda à pergunta: {pergunta}\"\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "pergunta = \"Qual é a capital da França?\"\n",
    "resposta = chain.run(pergunta)\n",
    "\n",
    "print(f\"Pergunta: {pergunta}\")\n",
    "print(f\"Resposta: {resposta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311e9902-412a-4d44-b28e-28dabb75dab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain transformers torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69cd2b5-df2b-4d83-94da-8d4d45b599d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Definir o modelo a ser utilizado\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"distilgpt2\",  # ID do modelo na Hugging Face\n",
    "    task=\"text-generation\",  # Tarefa de geração de texto\n",
    "    huggingfacehub_api_token = \"minha-chave\"\n",
    ")\n",
    "\n",
    "# Criar o template de prompt\n",
    "prompt_template = PromptTemplate(input_variables=[\"pergunta\"], template=\"Responda de forma clara à seguinte pergunta: {pergunta}\")\n",
    "\n",
    "# Criar o chain\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# Pergunta ao modelo\n",
    "pergunta = \"Quem foi o primeiro presidente dos Estados Unidos?\"\n",
    "\n",
    "# Obter a resposta\n",
    "resposta = chain.run(pergunta)\n",
    "\n",
    "# Exibir a resposta\n",
    "print(f\"Pergunta: {pergunta}\")\n",
    "print(f\"Resposta: {resposta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9734bbc5-8fcb-4fd1-8d9c-f45167c4186b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Definir o modelo a ser utilizado\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"distilgpt2\",  # ID do modelo na Hugging Face\n",
    "    task=\"text-generation\",  # Tarefa de geração de texto\n",
    "    huggingfacehub_api_token = \"minha-chave\"\n",
    ")\n",
    "\n",
    "# Criar o template de prompt\n",
    "prompt_template = PromptTemplate(input_variables=[\"pergunta\"], template=\"Responda de forma clara à seguinte pergunta: {pergunta}\")\n",
    "\n",
    "# Criar o chain\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# Pergunta ao modelo\n",
    "pergunta = \"Quala capital da França?\"\n",
    "\n",
    "# Obter a resposta\n",
    "resposta = chain.run(pergunta)\n",
    "\n",
    "# Exibir a resposta\n",
    "print(f\"Pergunta: {pergunta}\")\n",
    "print(f\"Resposta: {resposta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85289225-040b-437b-ae04-98d42bb3ad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdd348e-fe5b-435a-abd1-f4f1a80c3e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Defina o modelo\n",
    "generator = pipeline('text-generation', model='distilgpt2')\n",
    "\n",
    "# Geração de texto\n",
    "result = generator(\"Qual é a capital da França?\", max_length=50)\n",
    "\n",
    "# Exibir o resultado\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc840ad-3c21-48ac-9ced-3561373860f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Seu token de API do Hugging Face\n",
    "api_token = \"minha-chave\"\n",
    "\n",
    "# URL do modelo que você deseja usar (por exemplo, GPT-2)\n",
    "model_url = \"https://api-inference.huggingface.co/models/gpt2\"\n",
    "\n",
    "# Cabeçalhos necessários para passar o token de autenticação\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_token}\"\n",
    "}\n",
    "\n",
    "# Dados para enviar para o modelo (prompt)\n",
    "data = {\n",
    "    \"inputs\": \"Responda em apenas duas palavras: Quem foi o primeiro presidente dos Estados Unidos?\",\n",
    "}\n",
    "\n",
    "# Fazer a requisição à API do Hugging Face\n",
    "response = requests.post(model_url, headers=headers, json=data)\n",
    "\n",
    "# Verificar o status da resposta\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(\"Resposta:\", result[0][\"generated_text\"])\n",
    "else:\n",
    "    print(\"Erro na requisição:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd5e4f2-5edc-4178-bd7d-9830e03f28b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "\n",
    "# Carregar o modelo GPT-Neo e o tokenizer\n",
    "model_name = \"EleutherAI/gpt-neo-1.3B\"  # Você pode escolher outro modelo GPT-Neo\n",
    "model = GPTNeoForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Texto de entrada\n",
    "input_text = \"Quem foi o primeiro presidente dos Estados Unidos?\"\n",
    "\n",
    "# Tokenizar o texto de entrada\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Gerar resposta\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=100, num_return_sequences=1, temperature=0.7)\n",
    "\n",
    "# Decodificar e imprimir a resposta\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b752b63-2133-4cdf-8542-8bda99a5c902",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
